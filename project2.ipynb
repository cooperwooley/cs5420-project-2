{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5caeed4",
   "metadata": {},
   "source": [
    "# Project 2: NB Classifier\n",
    "\n",
    "### Course: CS 5420\n",
    "\n",
    "### Author: Cooper Wooley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504e5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import atexit\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514dee6",
   "metadata": {},
   "source": [
    "### Helper Functions for Managing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299de0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(zip_path):\n",
    "    # Create a temporary directory to extract into\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"dataset_\")\n",
    "\n",
    "    # Extract contents\n",
    "    with tarfile.open(zip_path, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(temp_dir)\n",
    "\n",
    "    # Register cleanup handler so even if program crashes, data is removed\n",
    "    atexit.register(lambda: cleanup_dataset(temp_dir))\n",
    "\n",
    "    # Find the first subdirectory inside extracted directory\n",
    "    contents = [os.path.join(temp_dir, d) for d in os.listdir(temp_dir)]\n",
    "    subdirs = [d for d in contents if os.path.isdir(d)]\n",
    "\n",
    "    if len(subdirs) == 1:\n",
    "        data_root = subdirs[0]\n",
    "    else:\n",
    "        data_root = temp_dir # fallback if already data root\n",
    "\n",
    "    return data_root\n",
    "\n",
    "def cleanup_dataset(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"Cleaned up dataset directory: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58d72",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6771b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to : /tmp/dataset_nmgrc56a/20_newsgroups\n"
     ]
    }
   ],
   "source": [
    "tar_path = \"20_newsgroups.tar.gz\"\n",
    "extracted_path = extract_dataset(tar_path)\n",
    "print(f\"Dataset extracted to : {extracted_path}\")\n",
    "\n",
    "def split_dataset(base_dir, train_ratio=0.5, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    for d in os.listdir(base_dir):\n",
    "        d_path = os.path.join(base_dir, d)\n",
    "        if not os.path.isdir(d_path):\n",
    "            continue\n",
    "\n",
    "        files = [\n",
    "            os.path.join(d_path, f)\n",
    "            for f in os.listdir(d_path)\n",
    "            if os.path.isfile(os.path.join(d_path, f))\n",
    "        ]\n",
    "\n",
    "        random.shuffle(files)\n",
    "        split_index = int(len(files) * train_ratio)\n",
    "\n",
    "        train_files.extend(files[:split_index])\n",
    "        test_files.extend(files[split_index:])\n",
    "        train_labels.extend([d] * split_index)\n",
    "        test_labels.extend([d] * (len(files) - split_index))\n",
    "\n",
    "    return train_files, test_files, train_labels, test_labels # train_X, test_Y, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c15d2-fdbd-4011-9087-9bc538f697b6",
   "metadata": {},
   "source": [
    "## NB Classifier\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6187e5ac-34a9-4c3c-8da3-f5dcae7fd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_files, train_labels, laplace_smooth=False):\n",
    "    vocab = set()\n",
    "    word_counts = defaultdict(Counter) # class: {word: count}\n",
    "    classes = set(train_labels)\n",
    "    total_docs = len(train_labels)\n",
    "\n",
    "    for path, label in zip(train_files, train_labels):\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            # Tokenize\n",
    "            words = re.findall(r'\\b\\w+\\b', f.read().lower())\n",
    "            vocab.update(w for w in words)\n",
    "            word_counts[label].update(words)\n",
    "\n",
    "    # Compute P(Y)\n",
    "    priors = {}\n",
    "    classes = set(train_labels)\n",
    "    for cls in classes:\n",
    "        priors[cls] = train_labels.count(cls) / total_docs\n",
    "\n",
    "    # Compute P(X|Y)\n",
    "    likelihoods = {}\n",
    "\n",
    "    for cls, words in word_counts.items():\n",
    "        total_words = sum(words.values())\n",
    "        class_likelihoods = {}\n",
    "        for word, count in words.items():\n",
    "            class_likelihoods[word] = (count + int(laplace_smooth)) / (total_words + (int(laplace_smooth) * len(vocab)))\n",
    "        likelihoods[cls] = class_likelihoods\n",
    "        \n",
    "    return priors, likelihoods, len(vocab), laplace_smooth, {cls: sum(words.values()) for cls, words in word_counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04259254-7aa0-4d62-932f-8a783fce5324",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94254dc1-37e2-48af-9b7c-65909ebce0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, priors, likelihoods, log_prob=False, vocab_size=None, laplace_smooth=False, word_counts=None):\n",
    "    with open(text, 'r', errors='ignore') as f:\n",
    "        words = re.findall(r'\\b\\w+\\b', f.read().lower())\n",
    "\n",
    "    score = {}\n",
    "    for cls, _ in priors.items():\n",
    "        score[cls] = math.log(priors[cls]) if log_prob else priors[cls]\n",
    "\n",
    "        for word in words:\n",
    "            if word in likelihoods[cls]:\n",
    "                if log_prob:\n",
    "                    score[cls] += math.log(likelihoods[cls][word])\n",
    "                else:\n",
    "                    score[cls] *= likelihoods[cls][word]\n",
    "            else:\n",
    "                # Handle unseen words with laplace smoothing\n",
    "                if laplace_smooth:\n",
    "                    if log_prob:\n",
    "                        score[cls] += math.log(1 / (word_counts[cls] + vocab_size))\n",
    "                    else:\n",
    "                        score[cls] *= 1 / (word_counts[cls] + vocab_size)\n",
    "\n",
    "    return max(score, key=score.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdb5a3-8704-43d4-b748-f1db7f993839",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19d370e-6b97-40bc-a9f9-2772eabe865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_files, test_labels, priors, likelihoods, log_prob=False, vocab_size=None, laplace_smooth=False, word_counts=None):\n",
    "    correct = 0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    for file, y in zip(test_files, test_labels):\n",
    "        y_hat = predict(file, priors, likelihoods, log_prob, vocab_size, laplace_smooth, word_counts)\n",
    "        class_total[y] += 1\n",
    "        if y_hat == y:\n",
    "            correct += 1\n",
    "            class_correct[y] += 1\n",
    "\n",
    "    overall_accuracy = correct / len(test_files)\n",
    "    class_accuracy = {}\n",
    "    for cls in class_total:\n",
    "        class_accuracy[cls] = class_correct[cls] / class_total[cls]\n",
    "    return overall_accuracy, class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0c5cd3-7493-422f-904a-eccc19ab319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0465\n",
      "\n",
      "Per-Class Accuracy:\n",
      "---------------------------------------------\n",
      "Class                          Accuracy       \n",
      "---------------------------------------------\n",
      "alt.atheism                    0.0000         \n",
      "comp.graphics                  0.0000         \n",
      "comp.os.ms-windows.misc        0.0000         \n",
      "comp.sys.ibm.pc.hardware       0.0000         \n",
      "comp.sys.mac.hardware          0.0040         \n",
      "comp.windows.x                 0.0020         \n",
      "misc.forsale                   0.0040         \n",
      "rec.autos                      0.0040         \n",
      "rec.motorcycles                0.0000         \n",
      "rec.sport.baseball             0.0020         \n",
      "rec.sport.hockey               0.9080         \n",
      "sci.crypt                      0.0000         \n",
      "sci.electronics                0.0060         \n",
      "sci.med                        0.0000         \n",
      "sci.space                      0.0000         \n",
      "soc.religion.christian         0.0000         \n",
      "talk.politics.guns             0.0000         \n",
      "talk.politics.mideast          0.0000         \n",
      "talk.politics.misc             0.0000         \n",
      "talk.religion.misc             0.0000         \n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_files, test_files, train_labels, test_labels = split_dataset(extracted_path)\n",
    "\n",
    "priors, likelihoods, _, _, _ = train_naive_bayes(train_files, train_labels)\n",
    "\n",
    "overall_acc_base, class_acc = evaluate(test_files, test_labels, priors, likelihoods)\n",
    "\n",
    "print(f\"Overall Accuracy: {overall_acc_base:.4f}\\n\")\n",
    "print(\"Per-Class Accuracy:\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Class':<30} {'Accuracy':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for cls in sorted(class_acc.keys()):\n",
    "    print(f\"{cls:<30} {class_acc[cls]:<15.4f}\")\n",
    "\n",
    "print(\"-\" * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cdf5b",
   "metadata": {},
   "source": [
    "## Expanding NB BoW Classifier\n",
    "\n",
    "Below is the implementation of the NB BoW Classifier utilizing Laplace smoothing when computing likelihoods, using log probability when classifying, and filtering words that are not in the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1b3896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ovearll Accuracy: 0.8647\n",
      "\n",
      "Difference of accuracy between classifiers: 0.8182\n"
     ]
    }
   ],
   "source": [
    "priors, likelihoods, vocab_size, laplace_smooth, word_counts = train_naive_bayes(train_files, train_labels, laplace_smooth=True)\n",
    "\n",
    "overall_acc_expand, _ = evaluate(test_files, test_labels, priors, likelihoods, log_prob=True, vocab_size=vocab_size, laplace_smooth=laplace_smooth, word_counts=word_counts)\n",
    "print(f\"Ovearll Accuracy: {overall_acc_expand:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference of accuracy between classifiers: {abs(overall_acc_expand - overall_acc_base):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc46f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up dataset directory: /tmp/dataset_nmgrc56a/20_newsgroups\n"
     ]
    }
   ],
   "source": [
    "cleanup_dataset(extracted_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
