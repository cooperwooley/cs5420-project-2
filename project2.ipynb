{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5caeed4",
   "metadata": {},
   "source": [
    "# Project 2: NB Classifier\n",
    "\n",
    "### Course: CS 5420\n",
    "\n",
    "### Author: Cooper Wooley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504e5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import atexit\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514dee6",
   "metadata": {},
   "source": [
    "### Helper Functions for Managing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299de0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(zip_path):\n",
    "    # Create a temporary directory to extract into\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"dataset_\")\n",
    "\n",
    "    # Extract contents\n",
    "    with tarfile.open(zip_path, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(temp_dir)\n",
    "\n",
    "    # Register cleanup handler so even if program crashes, data is removed\n",
    "    atexit.register(lambda: cleanup_dataset(temp_dir))\n",
    "\n",
    "    # Find the first subdirectory inside extracted directory\n",
    "    contents = [os.path.join(temp_dir, d) for d in os.listdir(temp_dir)]\n",
    "    subdirs = [d for d in contents if os.path.isdir(d)]\n",
    "\n",
    "    if len(subdirs) == 1:\n",
    "        data_root = subdirs[0]\n",
    "    else:\n",
    "        data_root = tempdir # fallback if already data root\n",
    "\n",
    "    return data_root\n",
    "\n",
    "def cleanup_dataset(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"Cleaned up dataset directory: {directory}\")\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     zip_path = \"path/to/your_dataset.zip\"\n",
    "\n",
    "#     # Step 1: Extract Data\n",
    "#     extracted_path = extract_dataset(zip_path)\n",
    "#     print(f\"Dataset extracted to: {extracted_path}\")\n",
    "\n",
    "#     # Step 2: load data, train models, etc.\n",
    "\n",
    "#     # Step 3: Cleanup Data\n",
    "#     # cleanup_dataset(extracted_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58d72",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6771b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to : /tmp/dataset_o01hu76p/20_newsgroups\n"
     ]
    }
   ],
   "source": [
    "tar_path = \"20_newsgroups.tar.gz\"\n",
    "extracted_path = extract_dataset(tar_path)\n",
    "print(f\"Dataset extracted to : {extracted_path}\")\n",
    "\n",
    "def split_dataset(base_dir, train_ratio=0.5, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    for d in os.listdir(base_dir):\n",
    "        d_path = os.path.join(base_dir, d)\n",
    "        if not os.path.isdir(d_path):\n",
    "            continue\n",
    "\n",
    "        files = [\n",
    "            os.path.join(d_path, f)\n",
    "            for f in os.listdir(d_path)\n",
    "            if os.path.isfile(os.path.join(d_path, f))\n",
    "        ]\n",
    "\n",
    "        random.shuffle(files)\n",
    "        split_index = int(len(files) * train_ratio)\n",
    "\n",
    "        train_files.extend(files[:split_index])\n",
    "        test_files.extend(files[split_index:])\n",
    "        train_categories.extend([d] * split_index)\n",
    "        test_categories.extend([d] * split_index)\n",
    "\n",
    "    return train_files, test_files, train_labels, test_labels # train_X, test_Y, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c15d2-fdbd-4011-9087-9bc538f697b6",
   "metadata": {},
   "source": [
    "## NB Classifier\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187e5ac-34a9-4c3c-8da3-f5dcae7fd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_files, train_labels):\n",
    "    vocab = set()\n",
    "    word_counts = defaultdict(Counter) # class: word: count\n",
    "    class_count = 20\n",
    "    total_docs = len(train_labels)\n",
    "\n",
    "    for path, label in zip(train_files, train_labels):\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            words = f.read().lower()\n",
    "            vocab.update(words)\n",
    "            word_counts[label].update(words)\n",
    "\n",
    "        # Computer P(Y)\n",
    "        priors = None\n",
    "\n",
    "        # Compute P(X|Y)\n",
    "        likelihoods = {}\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "        for cls, counts in word_counts.items():\n",
    "            likelihoods[cls] = None\n",
    "\n",
    "    return priors, likelihoods, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04259254-7aa0-4d62-932f-8a783fce5324",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94254dc1-37e2-48af-9b7c-65909ebce0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, priors, likelihoods, vocab):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdb5a3-8704-43d4-b748-f1db7f993839",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d370e-6b97-40bc-a9f9-2772eabe865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_files, test_labels, priors, likelihoods, vocab):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c5cd3-7493-422f-904a-eccc19ab319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_labels, test_labels = split_dataset(extracted_path)\n",
    "\n",
    "priors, likelihoods, vocab = train_naive_bayes(train_files, train_labels)\n",
    "\n",
    "accuracy = evaluate(test_files, test_labels, priors, likelihoods, vocab)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e539bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up dataset directory: /tmp/dataset_o01hu76p/20_newsgroups\n"
     ]
    }
   ],
   "source": [
    "cleanup_dataset(extracted_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
